{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Graph Neural Network\n",
    "\n",
    "We have found that GCN and GAT are CNN-like versions of graph neural networks. GGNN, on the other hand, is the RNN-like version of the node updating method. \n",
    "\n",
    "First, let's look at the message passing neural network (MPNN) framework. The MPNN framework updates the route node with the following formula. <br>\n",
    "\n",
    "$$ H_{i}^{(l+1)} = U(H_{i}^{(l)}, m^{(l+1)}) $$\n",
    "\n",
    "The i-th node, which is a route node, is newly updated through the message state, $m^{(i+1)}$ from the neighboring nodes and previous node state, $H^{(l)}$. <br>\n",
    "\n",
    "Updating message state can be written as a general formulation as follow.\n",
    "\n",
    "$$ m^{(l+1)} = \\sum_{j \\in N_{i}} M(H_i^{(l)}, H_j^{(l)}, e_{ij}) $$\n",
    "\n",
    "If we know the initial edge information - $e_{ij}$, we can update the message states differently for different relations, for example a single bond, a double bond and an aromatic bond will transfer a different message to the route node. <br>\n",
    "For simpliticy, we will only consider just connectivity between the node pairs, i.e.) $A_{ij} =1$ for connected node pairs, and zero otherwise.\n",
    "\n",
    "In GGNN framework, message function is defined as simple summation of the neighbor node states.\n",
    "\n",
    "$$ m^{(l+1)} = \\sum_{j \\in N_{i}} H_j^{(l)} $$\n",
    "\n",
    "And the gated recurrent unit (GRU) is used for the node updating. Finally, the node updating is re-written as follow.\n",
    "\n",
    "$$ H_i^{(l+1)} = GRU(H_i^{(l)}, \\sum_{j \\in N_i} H_i^{(l)}) $$\n",
    "\n",
    "We will implement the updating function in the GGNN framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lulu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ggnn(_X, _A, output_dim, num_layer):\n",
    "    num_nodes = int(_X.get_shape()[1])\n",
    "    input_dim = int(_X.get_shape()[2])\n",
    "    \n",
    "    if( input_dim != output_dim ):\n",
    "        _X = tf.layers.dense(_X, units=output_dim, use_bias=False)\n",
    "    \n",
    "    # Message state\n",
    "    _m = tf.matmul(_A, _X)\n",
    "    \n",
    "    # Update node state using GRU cell\n",
    "    X_total = []\n",
    "    cell = tf.contrib.rnn.GRUCell(output_dim, name='GRUcell'+str(num_layer))\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        mi = tf.expand_dims(_m[:,i,:],1)\n",
    "        hi = _X[:,i,:]\n",
    "        \n",
    "        _, _h = tf.nn.dynamic_rnn(cell, mi, initial_state=hi)\n",
    "        X_total.append(tf.expand_dims(_h, 1))\n",
    "        \n",
    "    output = tf.concat(X_total, 1)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if our code is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(?, 50, 32) dtype=float64>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float64, [None, 50, 58])\n",
    "A = tf.placeholder(tf.float64, [None, 50, 50])\n",
    "\n",
    "ggnn1 = ggnn(X, A, 32, 1)\n",
    "ggnn1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's right. We implemented a GGNN node updating method simply by using GRU cell.\n",
    "\n",
    "However, I have implemented it using the for statement here. I wonder if it can be implemented better by using tensorflow. In particular, when a graph neural network is applied to a molecule, a dynamic computational graph should be used when the number of atoms varies. If you know about this, please comment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
